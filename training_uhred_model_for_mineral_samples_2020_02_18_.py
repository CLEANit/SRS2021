# -*- coding: utf-8 -*-
"""Training UHRED model for mineral samples 2020-02-18 .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sJ-rcMp8B2PI3ivgIT-OsCjbTjqSZv0f

Importing libraries
"""

#Importing Libraries 
import torch 
from os import path
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from matplotlib import animation
import imageio
from pathlib import Path
import plotly.express as px
from scipy import ndimage
import torch.nn as nn
from torch.utils.data import DataLoader

"""Mounting a virtual drive on google drive."""

#Mounting a virtual drive on google drive. 
from google.colab import drive
drive.mount('/content/drive')

"""**Read_tiff** is a function that imports the hyperspectral SRS images then converts them to a numpy array.

Function argument is path of the file follows by file name.

"""

#Reading tiff image and store it in a numpy array
def read_tiff(path) :
    """
    path - Path of the multipage-tiff file
    """
    img = Image.open(path)
    images = []
    for i in range (img.n_frames) : 
        img.seek(i)
        images.append(np.array(img))
        
    return np.array(images)

#Importing the data, store it in to a numpy array  
data = read_tiff("/content/drive/MyDrive/Data/Li project-Raw data 4 fields of view/Scan 1 in Phase comp.tif") 
#printing size of the data file
print(data.shape)

"""Finding min, max and mean value of the input data."""

#Reading min, max and mean value of the imported data
mean_input = np.mean(data)
max_input = np.amax(data)
min_input = np.amin(data)
differenc = max_input-min_input
print ('Mean value is = %f \n Max value is = %f \n Min value is = %f \n difference of max and min value is = %f ' % (mean_input,max_input,min_input, differenc))

"""Ploting a slice of the input hyperspectral SRS image."""

#Plot a frame from the input dataset.
plt.imshow(data[151,:,:], cmap = 'Spectral_r')
plt.colorbar()

"""Reshaping of input dataset."""

#Reshaping input data.
data_N = np.empty((256,256,909))
for i in range(256):
  for j in range (256):
    for k in range (909):
      data_N [i,j,k] = data[k,i,j]

"""Normalizing input data (mean value of 0, standard deviation of 1)"""

data_N = (data_N-np.mean(data_N))/(np.max(data_N)-np.min(data_N))

#Checking the normalization
print(np.max(data_N)-np.min(data_N))

#Reading min, max and mean value of the imported data
mean_input_n = np.mean(data_N)
max_input_n = np.amax(data_N)
min_input_n = np.amin(data_N)
differenc = max_input_n-min_input_n
print('Mean value is = %f \n Max value is = %f \n Min value is = %f \n difference of max and min value is = %f ' % (mean_input_n,max_input_n,min_input_n, differenc))

"""Coverting the **numpy array** to the **pytorch tensor**.

Reshape it. 

Copy it on google **GPU**.



"""

#Converting numpy array to pyorch tensor and copy it on a GPU (CUDA).
input_t = data_N.reshape((65536,-1))
input_t = torch.from_numpy(input_t).float()
input_t = input_t.view(-1,1,909)
input_t = input_t.cuda()

#printing shape on input Torch tensor
print(input_t.shape)

"""## **Autoencoder model **"""

class Autoencoder(nn.Module):

  def __init__(self,zdims):
    super().__init__()
    self.zdims = zdims
    #Encoder layer
    self.encoder = nn.Sequential(nn.Conv1d(1,4,kernel_size = 4, stride = 3),
            nn.MaxPool1d(kernel_size = 4, stride = 3),
            nn.Tanh(),
                    
            
            nn.Conv1d(4,8,kernel_size=4, stride = 2),
            nn.MaxPool1d(kernel_size = 4,stride = 2),
            nn.Tanh(),
                     
            
            nn.Conv1d(8,12,kernel_size = 4, stride = 2),
            nn.MaxPool1d(kernel_size = 4,stride = 1),
            nn.Tanh(),

            nn.Conv1d(12,16,kernel_size = 4, stride = 1),
            nn.MaxPool1d(kernel_size = 3,stride = 1),
            nn.Tanh()
            
            )

    
    #Decoder layer
    self.decoder = nn.Sequential(nn.ConvTranspose1d(16,12,kernel_size = 5, stride = 2),
            nn.Tanh(),
            

            nn.ConvTranspose1d(12,8,kernel_size = 5,stride = 3),
            nn.Tanh(),
                        

            nn.ConvTranspose1d(8,4,kernel_size= 12,stride = 4),
            nn.Tanh(),

            nn.ConvTranspose1d(4,1,kernel_size = 18, stride = 9),
            nn.Tanh())
    

    #Deconv FC LAYER
    self.fc_e = nn.Linear(32,self.zdims)
    self.fc_d = nn.Linear(self.zdims,32)
    

  def encode(self,imgs):

      output = self.encoder(imgs)
      output = output.view(-1,32)
      output = self.fc_e(output)
      return output

  def decode(self,z):

      deconv_input = self.fc_d(z)
      deconv_input = deconv_input.view(-1,16,2)
      reconstructed_img = self.decoder(deconv_input)
      return reconstructed_img

  def forward(self,x):
    
      x = self.encode(x)
      reconstructed_img = self.decode(x)
      return reconstructed_img

#Printing the model architecture
print(Autoencoder(zdims = 20))

"""Using pytorch dataloader module to make minibatch dataset for training.

70% of data used for training and 30% for validation.
"""

#Loading input data on the dataloader module.
n_train = int(len(input_t)*0.70)
n_test = int(len(input_t)) - n_train
train_data_set, test_data_set = torch.utils.data.random_split(input_t, [n_train, n_test])
train_loader  = torch.utils.data.DataLoader(train_data_set, batch_size = 512, shuffle=True, drop_last=True)
test_loader  = torch.utils.data.DataLoader(test_data_set, batch_size = 512, shuffle=True, drop_last=True)

#Now see how test/train loader works/
for batch_idx, sample in enumerate(train_loader):
  inp = sample
  print(inp.size())
  
print ("//////////////////////////Test dataset//////////////////////////")
for batch_idx, sample in enumerate(test_loader):
  inp = sample
  print(inp.size())

"""Defining model parameters and optimization method.

Learning rate = 0.001, scheduler "StepLR" used. 

Optimizer = Adam

Criterion = MSE
"""

#Defining hyper parameters of the model
### num of epochs and also learning rate was determind experimentally () 
learning_rate = 1e-3
num_epochs = 400
model = Autoencoder(zdims = 20).cuda()
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,
weight_decay=1e-5)

###training loop, scheduler learning was used to find the best learning rate. in evert 250 epoch learning rate changes by the factor of 0.1
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=390, gamma=0.7, last_epoch=-1)

epoch_num = 0
train_error = []
test_error = []
best_error = 100
for epoch in range(num_epochs):
  print(scheduler.get_last_lr())
  loss_total = 0
  test_loss_total = 0
  model.train()
  for batch_idx, sample in enumerate(train_loader):
    inp = sample
    inp = inp.cuda()
    output = model(inp).cuda()
    loss = criterion(output, inp)
    loss_total += loss
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
  scheduler.step()
  loss_total = loss_total / (batch_idx+1)
  train_error.append(loss_total.item())

  model.eval()
  with torch.no_grad():
    for batch_idx, sample in enumerate(test_loader):
      inp = sample
      inp = inp.cuda()
      output = model(inp).cuda()
      loss = criterion(output, inp)
      test_loss_total += loss
    test_loss_total = test_loss_total / (batch_idx+1)
    if loss < best_error:
      best_error = loss
      best_epoch = epoch
      print('Best loss at epoch', best_epoch)
      model_save_name = 'Li 2021-02-19'
      path = F"/content/drive/My Drive/{model_save_name}" 
      torch.save(model.state_dict(), path)
    test_error.append(test_loss_total.item())
    if epoch%10 == 9:
      epoch_num+=epoch_num
      print (('\r Train Epoch : {}/{} \tLoss : {:.4f}'.format (epoch+1,num_epochs,loss_total))) 
      print (('\r Test Epoch : {}/{} \tLoss : {:.4f}'.format (epoch+1,num_epochs,test_loss_total))) 

print('best loss', best_error, ' at epoch ', best_epoch)
plt.plot(train_error)
plt.plot(test_error)
plt.xlabel('Number of iteration')
plt.ylabel('Loss (MSE)')
plt.title('Loss vs # of iterations')

#Ploting loss in log scale
fig = plt.figure()
ax = plt.subplot(111)
ax.plot(train_error, label = "Error of training set")
ax.plot(test_error, label = "Error of test set")
plt.yscale('log')
plt.xlabel("Itterations")
plt.ylabel("MSE(log Scale)")
ax.legend()
plt.show()

#Plot loss of test dataset
plt.plot(test_error)
plt.title("Testset error")
plt.yscale('log')
plt.xlabel('Iteration')
plt.ylabel('MSE (Log Scale)')

